{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Market Prediction .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDNzfaTvQXmp"
      },
      "source": [
        "library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMv3LclMN35u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944f57f6-9c7f-4c56-9dc9-d5128dbbacb5"
      },
      "source": [
        "import numpy as np \r\n",
        "import random\r\n",
        "import pandas as pd \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pandas import datetime\r\n",
        "import math, time\r\n",
        "import itertools\r\n",
        "from sklearn import preprocessing\r\n",
        "#from sklearn.preprocessing import MinMaxScaler\r\n",
        "import datetime\r\n",
        "from operator import itemgetter\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from math import sqrt\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.autograd import Variable\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MudKVN-QLW_a"
      },
      "source": [
        "#Device configuration\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7LztjTLQdCl"
      },
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYloyo52Qer2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "1f493fdb-04f3-4a7a-8538-6a1ee287a7dc"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Price.csv\")\r\n",
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-05</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-06</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-07</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-08</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851259</th>\n",
              "      <td>2016-12-30</td>\n",
              "      <td>ZBH</td>\n",
              "      <td>103.309998</td>\n",
              "      <td>103.199997</td>\n",
              "      <td>102.849998</td>\n",
              "      <td>103.930000</td>\n",
              "      <td>973800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851260</th>\n",
              "      <td>2016-12-30</td>\n",
              "      <td>ZION</td>\n",
              "      <td>43.070000</td>\n",
              "      <td>43.040001</td>\n",
              "      <td>42.689999</td>\n",
              "      <td>43.310001</td>\n",
              "      <td>1938100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851261</th>\n",
              "      <td>2016-12-30</td>\n",
              "      <td>ZTS</td>\n",
              "      <td>53.639999</td>\n",
              "      <td>53.529999</td>\n",
              "      <td>53.270000</td>\n",
              "      <td>53.740002</td>\n",
              "      <td>1701200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851262</th>\n",
              "      <td>2016-12-30</td>\n",
              "      <td>AIV</td>\n",
              "      <td>44.730000</td>\n",
              "      <td>45.450001</td>\n",
              "      <td>44.410000</td>\n",
              "      <td>45.590000</td>\n",
              "      <td>1380900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851263</th>\n",
              "      <td>2016-12-30</td>\n",
              "      <td>FTV</td>\n",
              "      <td>54.200001</td>\n",
              "      <td>53.630001</td>\n",
              "      <td>53.389999</td>\n",
              "      <td>54.480000</td>\n",
              "      <td>705100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>851264 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              date symbol        open  ...         low        high     volume\n",
              "0       2016-01-05   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n",
              "1       2016-01-06   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n",
              "2       2016-01-07   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n",
              "3       2016-01-08   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n",
              "4       2016-01-11   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n",
              "...            ...    ...         ...  ...         ...         ...        ...\n",
              "851259  2016-12-30    ZBH  103.309998  ...  102.849998  103.930000   973800.0\n",
              "851260  2016-12-30   ZION   43.070000  ...   42.689999   43.310001  1938100.0\n",
              "851261  2016-12-30    ZTS   53.639999  ...   53.270000   53.740002  1701200.0\n",
              "851262  2016-12-30    AIV   44.730000  ...   44.410000   45.590000  1380900.0\n",
              "851263  2016-12-30    FTV   54.200001  ...   53.389999   54.480000   705100.0\n",
              "\n",
              "[851264 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKbhcYzKGbo9",
        "outputId": "4aa2ae77-0618-421c-f549-050184afe032"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 851264 entries, 0 to 851263\n",
            "Data columns (total 7 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   date    851264 non-null  object \n",
            " 1   symbol  851264 non-null  object \n",
            " 2   open    851264 non-null  float64\n",
            " 3   close   851264 non-null  float64\n",
            " 4   low     851264 non-null  float64\n",
            " 5   high    851264 non-null  float64\n",
            " 6   volume  851264 non-null  float64\n",
            "dtypes: float64(5), object(2)\n",
            "memory usage: 45.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfJ2TX3YQrLG"
      },
      "source": [
        "PreProcess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW04A622QxyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6baedb93-7307-4f4c-83cf-96d87aba86c2"
      },
      "source": [
        "stock_df = df[df.symbol == 'YHOO'] #choose one stock\r\n",
        "print(stock_df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              date symbol       open  ...        low       high      volume\n",
            "713     2010-01-04   YHOO  16.940001  ...  16.879999  17.200001  16587400.0\n",
            "1181    2010-01-05   YHOO  17.219999  ...  17.000000  17.230000  11718100.0\n",
            "1649    2010-01-06   YHOO  17.170000  ...  17.070000  17.299999  16422000.0\n",
            "2117    2010-01-07   YHOO  16.809999  ...  16.570000  16.900000  31816300.0\n",
            "2585    2010-01-08   YHOO  16.680000  ...  16.620001  16.760000  15470000.0\n",
            "...            ...    ...        ...  ...        ...        ...         ...\n",
            "849257  2016-12-23   YHOO  38.459999  ...  38.369999  38.810001   3840100.0\n",
            "849757  2016-12-27   YHOO  38.590000  ...  38.500000  39.070000   4240300.0\n",
            "850257  2016-12-28   YHOO  39.119999  ...  38.709999  39.220001   4393500.0\n",
            "850757  2016-12-29   YHOO  38.759998  ...  38.480000  38.930000   4170200.0\n",
            "851257  2016-12-30   YHOO  38.720001  ...  38.430000  39.000000   6431600.0\n",
            "\n",
            "[1762 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "I9x7w7MvOPNH",
        "outputId": "94b4ac72-a1e3-4055-ab3e-fa11a9e53dc2"
      },
      "source": [
        "stock_df.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>849257</th>\n",
              "      <td>2016-12-23</td>\n",
              "      <td>YHOO</td>\n",
              "      <td>38.459999</td>\n",
              "      <td>38.660000</td>\n",
              "      <td>38.369999</td>\n",
              "      <td>38.810001</td>\n",
              "      <td>3840100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>849757</th>\n",
              "      <td>2016-12-27</td>\n",
              "      <td>YHOO</td>\n",
              "      <td>38.590000</td>\n",
              "      <td>38.919998</td>\n",
              "      <td>38.500000</td>\n",
              "      <td>39.070000</td>\n",
              "      <td>4240300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>850257</th>\n",
              "      <td>2016-12-28</td>\n",
              "      <td>YHOO</td>\n",
              "      <td>39.119999</td>\n",
              "      <td>38.730000</td>\n",
              "      <td>38.709999</td>\n",
              "      <td>39.220001</td>\n",
              "      <td>4393500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>850757</th>\n",
              "      <td>2016-12-29</td>\n",
              "      <td>YHOO</td>\n",
              "      <td>38.759998</td>\n",
              "      <td>38.639999</td>\n",
              "      <td>38.480000</td>\n",
              "      <td>38.930000</td>\n",
              "      <td>4170200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851257</th>\n",
              "      <td>2016-12-30</td>\n",
              "      <td>YHOO</td>\n",
              "      <td>38.720001</td>\n",
              "      <td>38.669998</td>\n",
              "      <td>38.430000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>6431600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              date symbol       open  ...        low       high     volume\n",
              "849257  2016-12-23   YHOO  38.459999  ...  38.369999  38.810001  3840100.0\n",
              "849757  2016-12-27   YHOO  38.590000  ...  38.500000  39.070000  4240300.0\n",
              "850257  2016-12-28   YHOO  39.119999  ...  38.709999  39.220001  4393500.0\n",
              "850757  2016-12-29   YHOO  38.759998  ...  38.480000  38.930000  4170200.0\n",
              "851257  2016-12-30   YHOO  38.720001  ...  38.430000  39.000000  6431600.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe_GjX3XObAA",
        "outputId": "4eeea5d6-0a0e-4ad7-f1ca-b7c42ef731a1"
      },
      "source": [
        "stock_df_dates = pd.to_datetime(df['date'])\r\n",
        "print(stock_df_dates)\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        2016-01-05\n",
            "1        2016-01-06\n",
            "2        2016-01-07\n",
            "3        2016-01-08\n",
            "4        2016-01-11\n",
            "            ...    \n",
            "851259   2016-12-30\n",
            "851260   2016-12-30\n",
            "851261   2016-12-30\n",
            "851262   2016-12-30\n",
            "851263   2016-12-30\n",
            "Name: date, Length: 851264, dtype: datetime64[ns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4Y5_lWPgSOO",
        "outputId": "ada11b44-f2c8-499b-9a1e-ebc88d1784ad"
      },
      "source": [
        "cols = list(stock_df)[2:7]\r\n",
        "print(cols)\r\n",
        "df_training = stock_df[cols].astype(float)\r\n",
        "print(df_training)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['open', 'close', 'low', 'high', 'volume']\n",
            "             open      close        low       high      volume\n",
            "713     16.940001  17.100000  16.879999  17.200001  16587400.0\n",
            "1181    17.219999  17.230000  17.000000  17.230000  11718100.0\n",
            "1649    17.170000  17.170000  17.070000  17.299999  16422000.0\n",
            "2117    16.809999  16.700001  16.570000  16.900000  31816300.0\n",
            "2585    16.680000  16.700001  16.620001  16.760000  15470000.0\n",
            "...           ...        ...        ...        ...         ...\n",
            "849257  38.459999  38.660000  38.369999  38.810001   3840100.0\n",
            "849757  38.590000  38.919998  38.500000  39.070000   4240300.0\n",
            "850257  39.119999  38.730000  38.709999  39.220001   4393500.0\n",
            "850757  38.759998  38.639999  38.480000  38.930000   4170200.0\n",
            "851257  38.720001  38.669998  38.430000  39.000000   6431600.0\n",
            "\n",
            "[1762 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apagLFZ5h5aE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eee896b-a88e-4b8b-c2f9-cc90f7a4733c"
      },
      "source": [
        "scaler = StandardScaler()\r\n",
        "scaler = scaler.fit(df_training)\r\n",
        "df_training_scaled = scaler.transform(df_training)\r\n",
        "\r\n",
        "df_training_scaled.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1762, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjnNh3WRtFdH"
      },
      "source": [
        "#splitting\r\n",
        "train_size = int(len(df_training_scaled)*.8)\r\n",
        "test_size = len(df_training_scaled)-train_size\r\n",
        "train_data = df_training_scaled[0:train_size,:]\r\n",
        "test_data = df_training_scaled[train_size:len(df_training_scaled),:] "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nripmIkhRggX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6972885a-af2a-430c-fa9b-5bb3fd901222"
      },
      "source": [
        "#data for lstm\r\n",
        "n_future = 1\r\n",
        "n_past = 14\r\n",
        "\r\n",
        "trainx = []\r\n",
        "trainy = []\r\n",
        "\r\n",
        "for i in range(n_past, len(train_data) - n_future +1):\r\n",
        "   trainx.append(train_data[i - n_past:i, 0:train_data.shape[1]])\r\n",
        "   trainy.append(train_data[i + n_future-1:i + n_future, 0])\r\n",
        "  \r\n",
        "trainX , trainY = np.array(trainx) , np.array(trainy)\r\n",
        "\r\n",
        "testx = []\r\n",
        "testy = []\r\n",
        "\r\n",
        "for i in range(n_past, len(test_data) - n_future +1):\r\n",
        "   testx.append(test_data[i - n_past:i, 0:train_data.shape[1]])\r\n",
        "   testy.append(test_data[i + n_future-1:i + n_future, 0])\r\n",
        "  \r\n",
        "testX , testY = np.array(testx) , np.array(testy)\r\n",
        "\r\n",
        "print(\"trainX shape == {}.\".format(trainX.shape))\r\n",
        "print(\"trainY shape == {}.\".format(trainY.shape))\r\n",
        "print(\"testX shape == {}.\".format(testX.shape))\r\n",
        "print(\"testY shape == {}.\".format(testY.shape))\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainX shape == (1395, 14, 5).\n",
            "trainY shape == (1395, 1).\n",
            "testX shape == (339, 14, 5).\n",
            "testY shape == (339, 1).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znkh4JNpnHM5"
      },
      "source": [
        "x_train = torch.from_numpy(trainX).type(torch.Tensor)\r\n",
        "x_test = torch.from_numpy(testX).type(torch.Tensor)\r\n",
        "y_train = torch.from_numpy(trainY).type(torch.Tensor)\r\n",
        "y_test = torch.from_numpy(testY).type(torch.Tensor)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqyIppkBBcdj",
        "outputId": "b1747223-7506-484a-bec6-3327308301b5"
      },
      "source": [
        "x_test.size()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([339, 14, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHd7WzKyn4zN"
      },
      "source": [
        "# Hyper-parameters \r\n",
        "input_size = 5\r\n",
        "seq_length = 14\r\n",
        "hidden_size = 2\r\n",
        "num_classes = 1\r\n",
        "batch_size = 16\r\n",
        "learning_rate = 0.001\r\n",
        "num_layers = 1\r\n",
        "num_timesteps = 14"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3amYn2rngmV"
      },
      "source": [
        "\r\n",
        "train = torch.utils.data.TensorDataset(x_train,y_train)\r\n",
        "test = torch.utils.data.TensorDataset(x_test,y_test)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train, \r\n",
        "                                           batch_size=batch_size, \r\n",
        "                                           shuffle=False)\r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test, \r\n",
        "                                          batch_size=batch_size, \r\n",
        "                                          shuffle=False)\r\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGEtMgaQ7gcr",
        "outputId": "cc79f136-fb38-475f-efc4-43f4930eb4bb"
      },
      "source": [
        "test_loader"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f37690897f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2RJHghhBC1m"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCQT6aKBwRaA"
      },
      "source": [
        "class MV_LSTM(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(MV_LSTM, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.num_layers = num_layers \r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.seq_length = seq_length\r\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) \r\n",
        "        #(batch, seq, input)\r\n",
        "        self.dropout = nn.Dropout(0.3)\r\n",
        "        self.fc = nn.Linear(hidden_size*seq_length, 1)\r\n",
        "\r\n",
        "    def init_hidden(self, batch_size):\r\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\r\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size) \r\n",
        "        self.hidden = (h0,c0)\r\n",
        "   \r\n",
        "    def forward(self, x):\r\n",
        "        batch_size, seq_length, _ = x.size()\r\n",
        "        lstm_out, self.hidden = self.lstm(x, self.hidden)\r\n",
        "        x = lstm_out.contiguous().view(batch_size, -1)\r\n",
        "        out = self.fc(x)\r\n",
        "        return out\r\n",
        "\r\n",
        "mv_model = MV_LSTM(input_size, hidden_size).to(device)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQA1qFKKG7JR",
        "outputId": "13620f87-4208-4b04-924f-aa80c73b871e"
      },
      "source": [
        "#create\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = torch.optim.Adam(mv_model.parameters(), lr=learning_rate)  \r\n",
        "print(mv_model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MV_LSTM(\n",
            "  (lstm): LSTM(5, 2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=28, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmxiCm0lDltA",
        "outputId": "dd21e36b-68fc-4e22-b0ff-f0599453c695"
      },
      "source": [
        "#Training\r\n",
        "train_episodes = 100\r\n",
        "hist = np.zeros(train_episodes)\r\n",
        "mv_model.train()\r\n",
        "for t in range(train_episodes):\r\n",
        "    for b in range(0,len(trainX),batch_size):\r\n",
        "        inpt = trainX[b:b+batch_size,:,:]\r\n",
        "        target =trainY[b:b+batch_size]    \r\n",
        "        \r\n",
        "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \r\n",
        "        y_batch = torch.tensor(target,dtype=torch.float32)\r\n",
        "       \r\n",
        "    \r\n",
        "        mv_model.init_hidden(x_batch.size(0))\r\n",
        "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \r\n",
        "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\r\n",
        "        output = mv_model(x_batch) \r\n",
        "        loss = criterion(output.view(-1), y_batch)\r\n",
        "        hist[t] = loss.item()\r\n",
        "        optimizer.zero_grad() \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()   \r\n",
        "\r\n",
        "\r\n",
        "    if t % 10 == 0 and t !=0:\r\n",
        "      print('epoch : ' , t , 'loss : ' , loss.item())\r\n",
        "\r\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([3, 1])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch :  10 loss :  0.0018423907458782196\n",
            "epoch :  20 loss :  0.0017257591243833303\n",
            "epoch :  30 loss :  0.0016366703202947974\n",
            "epoch :  40 loss :  0.0015701311640441418\n",
            "epoch :  50 loss :  0.0015208745608106256\n",
            "epoch :  60 loss :  0.0014845333062112331\n",
            "epoch :  70 loss :  0.0014579554554075003\n",
            "epoch :  80 loss :  0.001439007930457592\n",
            "epoch :  90 loss :  0.001426180824637413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd_tbfmYaq_m",
        "outputId": "f10325bb-b6c1-4e86-8d1e-3101e98d34c7"
      },
      "source": [
        " x_batch.size()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 14, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1--BSMIh4r2V"
      },
      "source": [
        "#def test():\r\n",
        " # correct = 0\r\n",
        "  #total = 0\r\n",
        "  #Tru = []\r\n",
        "  #Pred = []\r\n",
        "  #with torch.no_grad():\r\n",
        "   # for data in test_loader:\r\n",
        "   #   inpt, target = data\r\n",
        "   #   inpt = inpt.to(device)\r\n",
        "   #   target = target.to(device)\r\n",
        "   #   Tru.append(target.item())\r\n",
        "   #   out = mv_model(inpt)\r\n",
        "   #   _, preds = torch.max(out.data, 1)\r\n",
        "   #    total += target.size()\r\n",
        "   #   correct += (preds == target).sum().item()\r\n",
        "   # acc = correct/total\r\n",
        "  #return(Tru, Pred, correct, total, acc)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGRUX--xBYwB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeE_hRjh7OkG",
        "outputId": "128dae56-67a9-4830-f1bc-ef54abec4e9b"
      },
      "source": [
        "test_episodes = 100\r\n",
        "mv_model.eval()\r\n",
        "for t in range(test_episodes):\r\n",
        "    for b in range(0,len(testX),batch_size):\r\n",
        "        inpt_t = testX[b:b+batch_size,:,:]\r\n",
        "        target_t =testY[b:b+batch_size]    \r\n",
        "        \r\n",
        "        x_batch_t = torch.tensor(inpt_t,dtype=torch.float32)    \r\n",
        "        y_batch_t = torch.tensor(target_t,dtype=torch.float32)\r\n",
        "       \r\n",
        "    \r\n",
        "        mv_model.init_hidden(x_batch_t.size(0))\r\n",
        "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \r\n",
        "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\r\n",
        "        output_t = mv_model(x_batch_t) \r\n",
        "        loss = criterion(output_t.view(-1), y_batch_t)\r\n",
        "        \r\n",
        "        optimizer.zero_grad() \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()   \r\n",
        "\r\n",
        "\r\n",
        "    if t % 10 == 0 and t !=0:\r\n",
        "      print('epoch : ' , t , 'loss : ' , loss.item())\r\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([3, 1])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch :  10 loss :  0.0005161606823094189\n",
            "epoch :  20 loss :  0.00045598490396514535\n",
            "epoch :  30 loss :  0.0004289547214284539\n",
            "epoch :  40 loss :  0.00042912515345960855\n",
            "epoch :  50 loss :  0.00043777222163043916\n",
            "epoch :  60 loss :  0.00044951107702217996\n",
            "epoch :  70 loss :  0.0004624048597179353\n",
            "epoch :  80 loss :  0.0004757371498271823\n",
            "epoch :  90 loss :  0.0004892226424999535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "2bVTQF0pCukF",
        "outputId": "d0b7aff6-e69f-4407-f44a-2dbda9275b4d"
      },
      "source": [
        "plt.figure()\r\n",
        "plt.plot(hist, label=\"Training Loss\")\r\n",
        "plt.legend\r\n",
        "plt.show"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}